{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9e6e02",
   "metadata": {},
   "source": [
    "# HW1 – Clustering & Classification\n",
    "\n",
    "**Name:** Smridhi Patwari  \n",
    "**AndrewID:** smrihdip  \n",
    "**Date:** 09/12/2025\n",
    "\n",
    "**Objective:** Explore 2025 County Health Rankings data, find cluster patterns among counties, and build two models to predict **Premature Death** using **Community Conditions** features only. Document EDA, methods, results, and recommendations for **Allegheny County, PA**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e5dc6",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import f_classif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5547f",
   "metadata": {},
   "source": [
    "# 2. Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3043bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"analytic_data2025_v2.csv\"\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a13e55",
   "metadata": {},
   "source": [
    "# 2. Identifying raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381511b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Columns that does not contain the statecode ending with \"xxx_rawvalue\"\n",
    "statecode_row = df.iloc[0] #Identifying the row statecode row where \"xxx_rawvalue columns exist\"\n",
    "cols_to_drop = [col for i,col in enumerate(df.columns)\n",
    "                if i>=5 and not str(statecode_row[col]).endswith(\"_rawvalue\") ]\n",
    "df_raw =df.drop(columns = cols_to_drop)\n",
    "print(df_raw.shape)\n",
    "#to export data into CSV \n",
    "#df_raw.to_csv(\"raw_data_cleaned.csv\")\n",
    "df_raw.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02635b8f",
   "metadata": {},
   "source": [
    "# 3. Identifying relevant factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_cond_factors = [\n",
    "    \"Teen Births\",\n",
    "    \"Preventable Hospital Stays\",\n",
    "    \"Mammography Screening\",\n",
    "    \"Flu Vaccinations\",\n",
    "    \"Children in Poverty \",\n",
    "    \"Injury Deaths\",\n",
    "    \"Driving Alone to Work\",\n",
    "    \"Median Household Income\",\n",
    "    \"Suicides\",\n",
    "    \"Homicides\",\n",
    "    \"Firearm Fatalities\",\n",
    "    \"Drug Overdose Deaths\",\n",
    "    \"Motor Vehicle Crash Deaths\",\n",
    "    \"Reading Scores\",\n",
    "    \"Math Scores\"\n",
    "]\n",
    "\n",
    "\n",
    "factors_cols_to_drop = [col for i,col in enumerate(df_raw.columns)\n",
    "                            if i>=5 and not any (factor in col for factor in com_cond_factors)] #adapted from docs.python.org, python list comprehensions\n",
    "\n",
    "df_relevant_factors = df_raw.drop(columns=factors_cols_to_drop, index = 0) #drops all columns and the first row\n",
    "df_relevant_factors.to_csv(\"relevant_factors_data_cleaned.csv\")\n",
    "print(df_relevant_factors.shape)\n",
    "\n",
    "df_relevant_factors.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa3630",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "\n",
    "Since some columns are of object type and these columns also contain missing values, clustering cannot be performed yet. \n",
    "To ensure all columns are numeric, any non-conforming values must first be reinterpreted. Specifically, values that cannot be converted into \n",
    "numeric form should be replaced with NaN, so they can later be addressed using an appropriate imputation strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c18d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base = df_relevant_factors.copy()\n",
    "#print(X_base.dtypes)\n",
    "\n",
    "#Assigning missing data to NaN for it to be addressed later\n",
    "for col in X_base.columns[5:]:\n",
    "    X_base[col] = pd.to_numeric(X_base[col], errors=\"coerce\") # Google Search AI Overview - check for unique non-numeric values in a df column\n",
    "#print(X_base.dtypes) # Gives all float values \n",
    "\n",
    "\n",
    "# The following code base have been adapted from a chat with Google Gemini titled \"Gemini-Determining approach for visualizing data\" which will be attached with the submission\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(15, 12))\n",
    "# Flatten axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through columns and plot\n",
    "for i, col in enumerate(X_base.columns[5:]):\n",
    "    # Plot histogram on the i-th axis\n",
    "    sns.histplot(X_base[col], kde=True, ax=axes[i], color='skyblue')\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel('')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "data_columns = X_base.iloc[:, 5:]  \n",
    "\n",
    "#Creating Box Plots, adapted from session with chat gpt5- \"make me a python function that does boxplots for a bunch of columns in my dataset with seaborn\n",
    "def compute_and_plot_boxplot(data_columns):\n",
    "    fig,axes = plt.subplots(5, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    for i, column in enumerate(data_columns.columns):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(y=data_columns[column], ax=axes[i])\n",
    "            axes[i].set_title(f'{column}')\n",
    "            axes[i].set_ylabel('Values')\n",
    "\n",
    "            s = pd.to_numeric(data_columns[column], errors=\"coerce\")\n",
    "\n",
    "            # IQR values computation\n",
    "            q1 = s.quantile(0.25)\n",
    "            q3 = s.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            \n",
    "            axes[i].set_title(f'{column}')\n",
    "            axes[i].set_ylabel('Values')\n",
    "\n",
    "            # Annotate IQR inside the subplot\n",
    "            axes[i].text(\n",
    "                0.1, 0.95, f\"IQR={iqr:.2f}\", \n",
    "                ha='left', va='top', transform=axes[i].transAxes,\n",
    "                fontsize=9, color='red', weight='bold'\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "compute_and_plot_boxplot(data_columns)\n",
    "\n",
    "X_base.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e81a03",
   "metadata": {},
   "source": [
    "**Data Imputation Strategy:** \n",
    "By studying the histograms and boxplots, many variables exhibit right-skewed distributions (Suicides, Drug Overdose Deaths, Homicides) and substantial outliers (Preventable Hospital Stays, Median Household Income). The median imputation strategy was selected for this dataset since it is preserves the data characteristics better than the mean imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputation Strategy - Median \n",
    "for col in X_base.columns[5:]:\n",
    "    X_base[col] = X_base[col].fillna(X_base[col].median()) # Google Search AI Overview - How to fill NaN values in a df with median\n",
    "\n",
    "\n",
    "#Boxplot computation after Imputation\n",
    "data_columns_imp = X_base.iloc[:, 5:]  \n",
    "compute_and_plot_boxplot(data_columns_imp)\n",
    "X_base.head()\n",
    "\n",
    "X_base.to_csv(\"X_based_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31afec10",
   "metadata": {},
   "source": [
    "# 5. Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = X_base.copy()\n",
    "features = features_df.drop(columns = features_df.columns[:5])\n",
    "\n",
    "#From Recitations - Split between training and test data\n",
    "features_train, features_test, = train_test_split(\n",
    "    features, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(features_train.shape, features_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6f613",
   "metadata": {},
   "source": [
    "**Standardisation:** \n",
    " K-Means Algorithm is sensitve to the scale of data and hence it needs to be standardised. The RobhustScalar has been identified as the optimal scaling technique since the K-means algorithm minimises the sum of squared distances between points and their clusters which can cause the outliers to pull the center of the cluster further away. This is not ideal since it makes the cluster less compact and lose its intrepretebility leading to misleading representation of the typical profile of the group.\n",
    "\n",
    " The RobhustScalar method also uses the IQR range within its scaling process by centering the features along the median and scaling them by the IQR. This method allows the outliers to remain within the dataset while stil giving higher emphasis of scaling on the data within the IQR. \n",
    "\n",
    " From the Box plots and the distribution curves for the columns above, It can be established that the Homocide values have an IQR range of 0. This shows that there is no variability in the \"Homocide raw value\" data and as such it can be dropped and more relevant data can be concentrated upon for the clustering process. If the column is kept, it will result in errors when the column data is scaled using the RobhustScalar method since it divides by the IQR. Since IQR is 0, the entire Homocide data column will produce NaN values ans eventually break the K-Means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48cbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Homocide data from dataset\n",
    "cols_to_drop = [\"Homicides raw value\"]\n",
    "\n",
    "#Adopted from Recitations\n",
    "features_train = features_train.drop(columns=cols_to_drop, errors= \"ignore\")\n",
    "features_test  = features_test.drop(columns=cols_to_drop, errors = \"ignore\")\n",
    "\n",
    "\n",
    "scaler = RobustScaler(\n",
    "    with_centering=True,\n",
    "    with_scaling=True,\n",
    "    quantile_range=(25, 75) \n",
    ")\n",
    "features_train_std = scaler.fit_transform(features_train)\n",
    "features_test_std = scaler.transform(features_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb43c3",
   "metadata": {},
   "source": [
    "# Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a843959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from Recitations\n",
    "wcss = {}  # Within-cluster sum of squares\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, \n",
    "                       message='.*encountered in matmul.*')\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans_temp = KMeans(n_clusters=i, random_state=42).fit(features_train_std)  # Replace with the correct dataset if needed\n",
    "    wcss[i] = kmeans_temp.inertia_\n",
    "\n",
    "\n",
    "# Plot the WCSS values\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(wcss.keys(), wcss.values(), 'gs-')\n",
    "plt.xlabel(\"Values of 'k'\")\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dae3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://medium.com/analytics-vidhya/implementation-of-principal-component-analysis-pca-in-k-means-clustering-b4bc0aa79cb6\n",
    "\n",
    "pca = PCA(2)\n",
    "data = pca.fit_transform(features_train_std)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, random_state=42)\n",
    "label = kmeans.fit_predict(data)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "uniq = np.unique(label)\n",
    "for i in uniq:\n",
    "   plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)\n",
    "plt.scatter(centers[:,0], centers[:,1], marker=\"x\", color='k')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Adapted from ChatGPT 5 - applying ANOVA test to interpret the clustering results, the code generated by the LLM has been attached with the submission\n",
    "# ANOVA F-test: which features differ most across clusters? \n",
    "df_clusters = pd.DataFrame(features_train_std, columns=features_train.columns)\n",
    "df_clusters[\"cluster\"] = label\n",
    "\n",
    "X_anova = df_clusters.drop(columns=\"cluster\")\n",
    "y_anova = df_clusters[\"cluster\"]\n",
    "\n",
    "f_vals, p_vals = f_classif(X_anova, y_anova) #Adapted from: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
    "anova_importance = pd.Series(f_vals, index=X_anova.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"ANOVA top features:\\n\", anova_importance.head(8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689efef",
   "metadata": {},
   "source": [
    "Using the ANOVA test, we can check if the means of a feature differ significantly acorss clusters. The test produces F-values that determine how much a feature varies between the clusters relative to the variation within the clusters. \n",
    "From the results, the highest values are attributed to the Drug Overdose Deaths followed by Injury Deaths and Children in Poverty. \n",
    "\n",
    "Some implications from analysing the data are: \n",
    "1. some counties have much higher overdose death rates than others, being the a significant factor in differentiating across clusters. \n",
    "2. some counties differ strongly on how often they expereince injury-related deaths\n",
    "3. disadvantages in socioeconimic factors such as poverty and income play a key role in differentiating the clusters of counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b29d8",
   "metadata": {},
   "source": [
    "# 6. Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa650741",
   "metadata": {},
   "source": [
    "- Model 1 - Multi-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a77eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md\n",
    "\n",
    "#Target dataset\n",
    "MLR_features_df = X_base.drop(X_base.columns[:5], axis=1)\n",
    "col_names = MLR_features_df.columns.tolist()\n",
    "\n",
    "\n",
    "target_df = df_raw[\"Premature Death raw value\"].drop(index=0)\n",
    "#Convert valeus to numeric\n",
    "target_df = pd.to_numeric(target_df, errors=\"coerce\")\n",
    "#Fill NaN values with median\n",
    "target_df = target_df.fillna(target_df.median())\n",
    "target_df = target_df.to_frame(name=\"Premature Death raw value\")\n",
    "\n",
    "\n",
    "X = MLR_features_df.reset_index(drop=True)\n",
    "y = target_df.reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Predicting the test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "r2_MLR = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R² Score: {r2_MLR}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "plot_df = X_train.copy()\n",
    "\n",
    "plot_df[\"Premature Death raw value\"] = y_train.squeeze() #Adapted from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.squeeze.html\n",
    "\n",
    "n = len(col_names)\n",
    "ncols = 3\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 4*nrows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i,col in enumerate(col_names): #Adapted from : http://medium.com/@kathy.lu.rentals/visualizing-with-seaborn-regplot-2235ccbaedd4\n",
    "    sns.regplot(\n",
    "        data=plot_df,\n",
    "        x=col,\n",
    "        y=\"Premature Death raw value\",\n",
    "        ax=axes[i],\n",
    "        scatter_kws={\"alpha\":0.5, \"s\":18},  \n",
    "        line_kws={\"color\":\"red\"}  \n",
    "    )\n",
    "\n",
    "    #To get the slope and intercept of the graph\n",
    "    slope, intercept = np.polyfit(plot_df[col], plot_df[\"Premature Death raw value\"], 1) #Adapted from https://numpy.org/doc/2.0/reference/generated/numpy.polyfit.html\n",
    "\n",
    "    axes[i].text(\n",
    "        0.05, 0.95, f\"Slope = {slope:.2f}\", \n",
    "        transform=axes[i].transAxes, \n",
    "        ha=\"left\", va=\"top\", fontsize=9, color=\"blue\", weight=\"bold\"\n",
    "    )\n",
    "\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230534ab",
   "metadata": {},
   "source": [
    "From the MLR plots, the 5 most important factors influencing premature death are *Children in Poverty, Drug Overdose Deaths, Injury Deaths, Median Household Income, and Teen Births*. The *children in poverty* show the steepest positive slope, indicating that socioeconomic disadvantages are the strongest driver in premature deaths across counties. \n",
    "\n",
    "On the other hand, the graphs for *Reading Scores, Math Scores, and Median Household Income* show strong negative effects on premature deaths. This highlights the strong link between education, income, and health outcomes. Counties with better literacy and higher household income consistently see lower rates of premature death. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3de78f",
   "metadata": {},
   "source": [
    "- Model 2: Decision Tree Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Referenced from Google Search AI - give me a desition tree regressor algorithm in python for multiple factors\n",
    "\n",
    "regressor = DecisionTreeRegressor(criterion=\"squared_error\", random_state=0, max_depth=10)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae_tree = mean_absolute_error(y_test, y_pred)\n",
    "r2_tree   = r2_score(y_test, y_pred)   \n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae_tree}\")\n",
    "print(f\"R² Score: {r2_tree}\")\n",
    "\n",
    "#Adapted from: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "importances = regressor.feature_importances_\n",
    "predictions_series = pd.Series(importances, index=MLR_features_df.columns)\n",
    "top5 = predictions_series.head(5)\n",
    "print(\"\\nTop 5 factors (Decision Tree):\")\n",
    "print(top5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc857e5",
   "metadata": {},
   "source": [
    "Decision tree regressor (DRT), the top 5 most influencial factors are *Children in poverty, Driving ALone to work, Flu Vaccinations, Preventable Hospital Stays, and Mamography screenings.* The *Children in poverty* is given the prediction value of 44% making it sthe strongest factor contributing the premature deaths in counties. \n",
    "\n",
    "The other factors, though significanltly predicted to have less contribution on the premature deaths, still give important data on how they could affect premature deaths. The lowest predicted factors out of the top 5 factors are the Mamography Screening and the Flu Vaccinations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d43b0c",
   "metadata": {},
   "source": [
    "# Accuracy of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38632fc3",
   "metadata": {},
   "source": [
    "For the MLR model, the R² Score = 0.78 is > the R² Score = 0.6 of the DTR. The MLR model explains a greater amount of variance in the premature deaths than the DTR model. \n",
    "\n",
    "The Mean Squared Error (MSE) scores of the MLR model is MSE = 3.19 million which is < MSE = 5.89 million for the DTR model. This shows that the  DTR model has a higher prediction errors compared to the MLR model. \n",
    "\n",
    "The Mean Absolute Error(MAE) scores for the MLR algorithm is MAE = 1165 which is < MAE = 1469 for the DTR model. This means that the DTR model generally has higher prediction errors than that of the MLR model. \n",
    "\n",
    "From the analysis, it is clear that the MLR model is much more accurate than the DTR model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ec3fc",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b85d61",
   "metadata": {},
   "source": [
    "The most strongest predictor by both the models is the Children in Poverty leading to premature deaths in counties. From the analysis, there is also a strong co-relation between children being poor and the education, income, and health outcomes of populations in counties. This could be interpreted as children who are born in poverty tend to have lower access to quality education, limited job opportunities later in life, and poorer health outcomes overall.\n",
    "\n",
    "Allegheny County should as such prioritize interventions that reduce child poverty. Some programmes that could benefit children could be more assistance in education through social welfare programmes, increased access to affordable healthcare, and community support services. Such interventions can eventually directly impact reduction in poverty and improved opportunities available to children. The county can as such create long-term improvements in health poutcomes and premature reduce deaths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to HTML\n",
    "!jupyter nbconvert --to html IAI_HW1_smridhip.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa4188",
   "metadata": {},
   "source": [
    "1. Corelation matrix, feature selection routines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
